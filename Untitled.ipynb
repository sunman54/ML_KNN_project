{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12c3af38",
   "metadata": {},
   "source": [
    "#                  <font color=red>       BBM409 : Introduction to Machine Learning Lab - Assignment 1</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e082dbc",
   "metadata": {},
   "source": [
    "<img src=\"logo.png\" width=300 height=200 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792aa803",
   "metadata": {},
   "source": [
    "## <font color=green><center>Berra Nur SARI - 21727671 <br> Melih SUNMAN - 21827809</center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e48160",
   "metadata": {},
   "source": [
    "#   <font color=blue>PART 1: Glass Material Classification</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ab22a1",
   "metadata": {},
   "source": [
    "## Abstract :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf72ed63",
   "metadata": {},
   "source": [
    "In the first part of this project, the KNN machine learning algorithm was used for the glass dataset. The accuracies of the model were calculated on 5-fold cross validation with different k parameters (1,3,5,7,9) of the model's different k-NN(with normalization and without normalization) and weighted k-NN (with normalization and without normalization) models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b1575c",
   "metadata": {},
   "source": [
    "## Code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1180b29f",
   "metadata": {},
   "source": [
    "Required libraries are imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9ead5319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bfb2f6",
   "metadata": {},
   "source": [
    "KNN Classification Class  \n",
    "\n",
    "1. \"fit(self, x, y)\" function: Takes which features to train and their labels as parameters.  \n",
    "\n",
    "2. \"predict(self, test_validation)\" function: It calls the _predict(self,x) function for each data to be tested and stores the predictions in an array. This returns the predicted array.\n",
    "\n",
    "3. \"predict_weighted(self, test_validation)\" function: It calls the _predict_weighted(self,x,hyper_parameter) function for each data to be tested and stores the predictions in an array. This returns the predicted array.\n",
    "\n",
    "4. \"euclidean_distance(self, row1, row2)\" function: Calculates the distance between two given vectors with Euclidean distance.\n",
    "\n",
    "5. \"manhattan_distance(self, row1, row2)\" function: Calculates the distance between two given vectors with Manahttan distance.\n",
    "\n",
    "6. \"_predict(self, x)\" function: With distance array, for each sample in the given training set, it holds the distance of a particular sample from other samples. And then, create k_indices array which holding the indexes of the vectors with the closest distance(as many as k). Next, create k_nearest_labels array which holding the values of the vectors with the closest distance. Then, with most_common, it is determined which label is repeated the most and returned.\n",
    "\n",
    "7. \"_predict_weighted(self, x , hyper_parameter)\" function: With distance array, for each sample in the given training set, it holds the distance of a particular sample from other samples. And then, create k_indices array which holding the indexes of the vectors with the closest distance(as many as k). And then, create k_nearest_labels array which holding the values of the vectors with the closest distance.Also, create k_distances array which holding the distances of the vectors with the closest distance. And create a dictionary to match distances and labels. The hyper parameter was created to avoid offsets of 0.0. We can explain it like this, if the distance is zero, the 1/d calculation cannot be made and in fact the closest value is considered invalid. For this reason, the distance measure as much as the entered hyper parameter is added to all the distances in the data set. In this way, the closest value actually has the shortest distance and has the greatest weight through the 1/d expression. After adding the entered hyperparameter to all distances, the corresponding weights for all labels were calculated and recorded in a dictionary. Then the label with the greatest weight is returned.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9dd8afb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN_Classification:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.x_train = x\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, test_validation):\n",
    "        predicted_labels = [self._predict(x) for x in test_validation]\n",
    "        return np.array(predicted_labels)\n",
    "\n",
    "    def predict_weighted(self, test_validation , hyper_parameter):\n",
    "        predicted_labels = [self._predict_weighted(x , hyper_parameter) for x in test_validation]\n",
    "        return np.array(predicted_labels)\n",
    "\n",
    "    def euclidean_distance(self, row1, row2):\n",
    "        distance = 0.0\n",
    "        for i in range(len(row1) - 1):\n",
    "            distance += (row1[i] - row2[i]) ** 2\n",
    "        return sqrt(distance)\n",
    "\n",
    "    def manhattan_distance(self, row1, row2):\n",
    "        distance = 0.0\n",
    "        for i in range(len(row1) - 1):\n",
    "            distance += abs(row1[i] - row2[i])\n",
    "        return distance\n",
    "\n",
    "    def _predict(self, x):\n",
    "        distances = [self.euclidean_distance(x, x_train) for x_train in self.x_train]\n",
    "        #distances = [self.manhattan_distance(x, x_train) for x_train in self.x_train]\n",
    "\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "\n",
    "        most_common = Counter(k_nearest_labels).most_common(1) \n",
    "        return most_common[0][0]\n",
    "\n",
    "    def _predict_weighted(self, x , hyper_parameter):\n",
    "        distances = [self.euclidean_distance(x, x_train) for x_train in self.x_train]\n",
    "        # distances = [self.manhattan_distance(x, x_train) for x_train in self.x_train]\n",
    "\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        k_distances = [distances[i] for i in k_indices]\n",
    "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "\n",
    "        dict = {}\n",
    "\n",
    "        # add a small value in all label's distance and avoid 0 distance\n",
    "        for i in range(len(k_distances)):\n",
    "            k_distances[i] = k_distances[i] + hyper_parameter\n",
    "\n",
    "        for i in range(len(k_nearest_labels)):\n",
    "            if k_nearest_labels[i] in dict:\n",
    "                dict[k_nearest_labels[i]] += 1 / k_distances[i]\n",
    "            elif k_nearest_labels[i] not in dict:\n",
    "                dict[k_nearest_labels[i]] = 1 / k_distances[i]\n",
    "\n",
    "        return max(dict, key=dict.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2b405fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading csv file with pandas\n",
    "df = pd.read_csv('glass.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "10195604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the data and convert numpy\n",
    "df = df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f56d6b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle\n",
    "np.random.shuffle(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3a63a268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the data as features and labels\n",
    "features = df[:, :-1]\n",
    "labels = df[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0be7d3",
   "metadata": {},
   "source": [
    "Used min-max normalization on the features of samples to re-scale each feature (feature/attribute column on data) between (0-1) range. For this, each column was handled separately, the minimum and maximum data in the column were obtained, and the data with the normalization algorithm were re-recorded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4ff9558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(df, features):\n",
    "    for feature in range(df.shape[1] - 1):\n",
    "        minvalue = features[:, feature].min()\n",
    "        maxvalue = features[:, feature].max()\n",
    "        for i in range(len(df)):\n",
    "            features[i, feature] = (features[i, feature] - minvalue) / (maxvalue - minvalue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fe5cbf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating normalized features\n",
    "normalized_features = features.copy()\n",
    "normalization(df, normalized_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0dde6ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSize = len(df) \n",
    "numberOfCrossSize = int(len(df) / 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c6e262",
   "metadata": {},
   "source": [
    "For each desired k value (1,3,5,7,9), 5-fold cross validation was performed separately and the results were printed. First of all, validate and train sets were determined. Then, \"unnormalized and unweighted knn\" , \"normalized and unweighted knn\" , \"unnormalized and weighted knn\" and \"normalized and weighted knn\" algorithms were run with the training data set, respectively, and their accuracy values were reached. Each accuracy value and its average are printed on the screen. <br><br>\n",
    "In the creation phase of each model, first the model was created with the k value, the training data was given to the model, and then it was expected to make predictions from the model. Meanwhile, missing values were also kept in a dictionary and printed after each prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e2d6aa8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy without normalization for cross validation =  0  ->  66.66666666666666\n",
      "Computation time: 0.0359959602355957\n",
      "Missing values:  {1.0: [3.0], 3.0: [1.0, 2.0, 1.0, 2.0], 2.0: [6.0, 1.0, 1.0, 1.0, 5.0, 1.0, 1.0, 1.0], 6.0: [7.0]}\n",
      "\n",
      "accuracy with normalization for cross validation =  0  ->  69.04761904761905\n",
      "Computation time: 0.03599739074707031\n",
      "Missing values:  {1.0: [3.0], 3.0: [1.0, 2.0, 1.0, 2.0], 2.0: [6.0, 1.0, 1.0, 5.0, 1.0, 1.0, 1.0], 6.0: [7.0]}\n",
      "\n",
      "weighted accuracy without normalization for cross validation =  0  ->  66.66666666666666\n",
      "Computation time: 0.03601884841918945\n",
      "Missing values:  {1.0: [3.0], 3.0: [1.0, 2.0, 1.0, 2.0], 2.0: [6.0, 1.0, 1.0, 1.0, 5.0, 1.0, 1.0, 1.0], 6.0: [7.0]}\n",
      "\n",
      "weighted accuracy with normalization for cross validation =  0  ->  69.04761904761905\n",
      "Computation time: 0.03500771522521973\n",
      "Missing values:  {1.0: [3.0], 3.0: [1.0, 2.0, 1.0, 2.0], 2.0: [6.0, 1.0, 1.0, 5.0, 1.0, 1.0, 1.0], 6.0: [7.0]}\n",
      "\n",
      "accuracy without normalization for cross validation =  1  ->  76.19047619047619\n",
      "Computation time: 0.03600192070007324\n",
      "Missing values:  {1.0: [3.0, 2.0, 2.0, 2.0], 2.0: [1.0, 1.0, 5.0], 3.0: [1.0], 7.0: [3.0], 5.0: [7.0]}\n",
      "\n",
      "accuracy with normalization for cross validation =  1  ->  76.19047619047619\n",
      "Computation time: 0.03600788116455078\n",
      "Missing values:  {3.0: [1.0], 1.0: [3.0, 2.0, 2.0, 2.0], 2.0: [1.0, 1.0, 5.0], 7.0: [2.0], 5.0: [7.0]}\n",
      "\n",
      "weighted accuracy without normalization for cross validation =  1  ->  76.19047619047619\n",
      "Computation time: 0.035703420639038086\n",
      "Missing values:  {1.0: [3.0, 2.0, 2.0, 2.0], 2.0: [1.0, 1.0, 5.0], 3.0: [1.0], 7.0: [3.0], 5.0: [7.0]}\n",
      "\n",
      "weighted accuracy with normalization for cross validation =  1  ->  76.19047619047619\n",
      "Computation time: 0.03600192070007324\n",
      "Missing values:  {3.0: [1.0], 1.0: [3.0, 2.0, 2.0, 2.0], 2.0: [1.0, 1.0, 5.0], 7.0: [2.0], 5.0: [7.0]}\n",
      "\n",
      "accuracy without normalization for cross validation =  2  ->  71.42857142857143\n",
      "Computation time: 0.03500795364379883\n",
      "Missing values:  {3.0: [1.0, 1.0, 1.0, 1.0], 1.0: [2.0, 3.0, 2.0], 2.0: [5.0, 1.0], 6.0: [2.0], 5.0: [2.0], 7.0: [5.0]}\n",
      "\n",
      "accuracy with normalization for cross validation =  2  ->  83.33333333333334\n",
      "Computation time: 0.03701448440551758\n",
      "Missing values:  {3.0: [1.0, 1.0, 1.0], 1.0: [2.0], 2.0: [5.0], 6.0: [2.0], 7.0: [5.0]}\n",
      "\n",
      "weighted accuracy without normalization for cross validation =  2  ->  71.42857142857143\n",
      "Computation time: 0.03500795364379883\n",
      "Missing values:  {3.0: [1.0, 1.0, 1.0, 1.0], 1.0: [2.0, 3.0, 2.0], 2.0: [5.0, 1.0], 6.0: [2.0], 5.0: [2.0], 7.0: [5.0]}\n",
      "\n",
      "weighted accuracy with normalization for cross validation =  2  ->  83.33333333333334\n",
      "Computation time: 0.03600168228149414\n",
      "Missing values:  {3.0: [1.0, 1.0, 1.0], 1.0: [2.0], 2.0: [5.0], 6.0: [2.0], 7.0: [5.0]}\n",
      "\n",
      "accuracy without normalization for cross validation =  3  ->  73.80952380952381\n",
      "Computation time: 0.036014556884765625\n",
      "Missing values:  {2.0: [1.0, 1.0, 3.0, 3.0], 1.0: [7.0, 3.0, 2.0, 3.0], 5.0: [6.0], 3.0: [1.0], 7.0: [6.0]}\n",
      "\n",
      "accuracy with normalization for cross validation =  3  ->  83.33333333333334\n",
      "Computation time: 0.03599357604980469\n",
      "Missing values:  {2.0: [3.0, 1.0, 1.0], 5.0: [6.0], 3.0: [1.0], 7.0: [6.0], 1.0: [2.0]}\n",
      "\n",
      "weighted accuracy without normalization for cross validation =  3  ->  73.80952380952381\n",
      "Computation time: 0.03602099418640137\n",
      "Missing values:  {2.0: [1.0, 1.0, 3.0, 3.0], 1.0: [7.0, 3.0, 2.0, 3.0], 5.0: [6.0], 3.0: [1.0], 7.0: [6.0]}\n",
      "\n",
      "weighted accuracy with normalization for cross validation =  3  ->  83.33333333333334\n",
      "Computation time: 0.034995079040527344\n",
      "Missing values:  {2.0: [3.0, 1.0, 1.0], 5.0: [6.0], 3.0: [1.0], 7.0: [6.0], 1.0: [2.0]}\n",
      "\n",
      "accuracy without normalization for cross validation =  4  ->  62.7906976744186\n",
      "Computation time: 0.03702116012573242\n",
      "Missing values:  {3.0: [1.0], 1.0: [2.0, 2.0, 3.0, 3.0, 2.0], 2.0: [1.0, 5.0, 1.0, 1.0, 3.0, 1.0, 3.0], 6.0: [5.0], 7.0: [6.0, 2.0]}\n",
      "\n",
      "accuracy with normalization for cross validation =  4  ->  74.4186046511628\n",
      "Computation time: 0.036008358001708984\n",
      "Missing values:  {2.0: [1.0, 5.0, 7.0, 3.0], 6.0: [5.0], 1.0: [2.0, 3.0, 3.0, 2.0], 7.0: [2.0, 2.0]}\n",
      "\n",
      "weighted accuracy without normalization for cross validation =  4  ->  62.7906976744186\n",
      "Computation time: 0.03900909423828125\n",
      "Missing values:  {3.0: [1.0], 1.0: [2.0, 2.0, 3.0, 3.0, 2.0], 2.0: [1.0, 5.0, 1.0, 1.0, 3.0, 1.0, 3.0], 6.0: [5.0], 7.0: [6.0, 2.0]}\n",
      "\n",
      "weighted accuracy with normalization for cross validation =  4  ->  74.4186046511628\n",
      "Computation time: 0.037001848220825195\n",
      "Missing values:  {2.0: [1.0, 5.0, 7.0, 3.0], 6.0: [5.0], 1.0: [2.0, 3.0, 3.0, 2.0], 7.0: [2.0, 2.0]}\n",
      "\n",
      "for k =  1\n",
      "average_accuracy   70.17718715393133\n",
      "average_accuracy with normalization   77.26467331118495\n",
      "average_accuracy_weighted without normalization   70.17718715393133\n",
      "average_accuracy_weighted with normalization   77.26467331118495\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------------- \n",
      "\n",
      "accuracy without normalization for cross validation =  0  ->  61.904761904761905\n",
      "Computation time: 0.03600788116455078\n",
      "Missing values:  {1.0: [3.0, 2.0], 3.0: [1.0, 2.0, 1.0, 2.0], 2.0: [6.0, 1.0, 1.0, 1.0, 5.0, 1.0, 1.0, 1.0, 1.0], 6.0: [7.0]}\n",
      "\n",
      "accuracy with normalization for cross validation =  0  ->  64.28571428571429\n",
      "Computation time: 0.0370020866394043\n",
      "Missing values:  {1.0: [3.0, 2.0], 3.0: [1.0, 2.0, 1.0, 2.0], 2.0: [5.0, 1.0, 1.0, 5.0, 1.0, 1.0, 1.0, 1.0], 6.0: [7.0]}\n",
      "\n",
      "weighted accuracy without normalization for cross validation =  0  ->  61.904761904761905\n",
      "Computation time: 0.03672146797180176\n",
      "Missing values:  {1.0: [3.0, 2.0], 3.0: [1.0, 2.0, 1.0, 2.0], 2.0: [6.0, 1.0, 1.0, 1.0, 5.0, 1.0, 1.0, 1.0, 1.0], 6.0: [7.0]}\n",
      "\n",
      "weighted accuracy with normalization for cross validation =  0  ->  64.28571428571429\n",
      "Computation time: 0.039002418518066406\n",
      "Missing values:  {1.0: [3.0, 2.0], 3.0: [1.0, 2.0, 1.0, 2.0], 2.0: [5.0, 1.0, 1.0, 5.0, 1.0, 1.0, 1.0, 1.0], 6.0: [7.0]}\n",
      "\n",
      "accuracy without normalization for cross validation =  1  ->  64.28571428571429\n",
      "Computation time: 0.036008358001708984\n",
      "Missing values:  {3.0: [1.0, 1.0], 2.0: [6.0, 1.0, 1.0, 5.0, 1.0], 1.0: [3.0, 3.0, 2.0, 2.0, 2.0, 2.0], 7.0: [3.0], 5.0: [7.0]}\n",
      "\n",
      "accuracy with normalization for cross validation =  1  ->  66.66666666666666\n",
      "Computation time: 0.03600192070007324\n",
      "Missing values:  {2.0: [6.0, 1.0, 1.0, 5.0], 1.0: [3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0], 3.0: [2.0], 7.0: [1.0], 5.0: [7.0]}\n",
      "\n",
      "weighted accuracy without normalization for cross validation =  1  ->  66.66666666666666\n",
      "Computation time: 0.03601431846618652\n",
      "Missing values:  {3.0: [1.0, 1.0], 2.0: [6.0, 1.0, 1.0, 5.0, 1.0], 1.0: [3.0, 3.0, 2.0, 2.0, 2.0], 7.0: [3.0], 5.0: [7.0]}\n",
      "\n",
      "weighted accuracy with normalization for cross validation =  1  ->  66.66666666666666\n",
      "Computation time: 0.0357363224029541\n",
      "Missing values:  {2.0: [6.0, 1.0, 1.0, 5.0], 1.0: [3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0], 3.0: [2.0], 7.0: [1.0], 5.0: [7.0]}\n",
      "\n",
      "accuracy without normalization for cross validation =  2  ->  69.04761904761905\n",
      "Computation time: 0.0360109806060791\n",
      "Missing values:  {3.0: [1.0, 1.0, 1.0], 1.0: [2.0, 3.0, 2.0], 2.0: [5.0, 1.0, 1.0], 6.0: [2.0], 5.0: [2.0, 2.0], 7.0: [5.0]}\n",
      "\n",
      "accuracy with normalization for cross validation =  2  ->  76.19047619047619\n",
      "Computation time: 0.03702569007873535\n",
      "Missing values:  {3.0: [1.0, 1.0, 1.0], 1.0: [2.0], 2.0: [5.0, 6.0, 1.0], 6.0: [5.0], 7.0: [5.0], 5.0: [2.0]}\n",
      "\n",
      "weighted accuracy without normalization for cross validation =  2  ->  69.04761904761905\n",
      "Computation time: 0.03600168228149414\n",
      "Missing values:  {3.0: [1.0, 1.0, 1.0], 1.0: [2.0, 3.0, 2.0], 2.0: [5.0, 1.0, 1.0], 6.0: [2.0], 5.0: [2.0, 2.0], 7.0: [5.0]}\n",
      "\n",
      "weighted accuracy with normalization for cross validation =  2  ->  76.19047619047619\n",
      "Computation time: 0.03502035140991211\n",
      "Missing values:  {3.0: [1.0, 1.0, 1.0], 1.0: [2.0], 2.0: [5.0, 6.0, 1.0], 6.0: [5.0], 7.0: [5.0], 5.0: [2.0]}\n",
      "\n",
      "accuracy without normalization for cross validation =  3  ->  80.95238095238095\n",
      "Computation time: 0.03600740432739258\n",
      "Missing values:  {2.0: [1.0, 1.0, 1.0], 1.0: [7.0, 2.0], 6.0: [7.0], 3.0: [1.0], 7.0: [6.0]}\n",
      "\n",
      "accuracy with normalization for cross validation =  3  ->  85.71428571428571\n",
      "Computation time: 0.0370025634765625\n",
      "Missing values:  {2.0: [3.0, 1.0, 1.0], 3.0: [1.0], 7.0: [6.0], 1.0: [2.0]}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted accuracy without normalization for cross validation =  3  ->  80.95238095238095\n",
      "Computation time: 0.037009239196777344\n",
      "Missing values:  {2.0: [1.0, 1.0, 1.0], 1.0: [7.0, 2.0], 6.0: [7.0], 3.0: [1.0], 7.0: [6.0]}\n",
      "\n",
      "weighted accuracy with normalization for cross validation =  3  ->  85.71428571428571\n",
      "Computation time: 0.036000967025756836\n",
      "Missing values:  {2.0: [3.0, 1.0, 1.0], 3.0: [1.0], 7.0: [6.0], 1.0: [2.0]}\n",
      "\n",
      "accuracy without normalization for cross validation =  4  ->  62.7906976744186\n",
      "Computation time: 0.04602336883544922\n",
      "Missing values:  {3.0: [1.0], 1.0: [2.0, 2.0, 3.0, 2.0], 2.0: [1.0, 1.0, 5.0, 1.0, 1.0, 3.0], 6.0: [2.0], 7.0: [1.0, 2.0, 2.0], 5.0: [2.0]}\n",
      "\n",
      "accuracy with normalization for cross validation =  4  ->  67.44186046511628\n",
      "Computation time: 0.03700828552246094\n",
      "Missing values:  {2.0: [1.0, 1.0, 5.0, 1.0, 7.0, 3.0], 6.0: [5.0], 1.0: [2.0, 3.0, 3.0, 2.0], 7.0: [2.0, 2.0], 5.0: [2.0]}\n",
      "\n",
      "weighted accuracy without normalization for cross validation =  4  ->  62.7906976744186\n",
      "Computation time: 0.0359954833984375\n",
      "Missing values:  {3.0: [1.0], 1.0: [2.0, 2.0, 3.0, 2.0], 2.0: [1.0, 1.0, 5.0, 1.0, 1.0, 3.0], 6.0: [2.0], 7.0: [1.0, 2.0, 2.0], 5.0: [2.0]}\n",
      "\n",
      "weighted accuracy with normalization for cross validation =  4  ->  67.44186046511628\n",
      "Computation time: 0.03693962097167969\n",
      "Missing values:  {2.0: [1.0, 1.0, 5.0, 1.0, 7.0, 3.0], 6.0: [5.0], 1.0: [2.0, 3.0, 3.0, 2.0], 7.0: [2.0, 2.0], 5.0: [2.0]}\n",
      "\n",
      "for k =  3\n",
      "average_accuracy   67.79623477297896\n",
      "average_accuracy with normalization   72.05980066445184\n",
      "average_accuracy_weighted without normalization   68.27242524916943\n",
      "average_accuracy_weighted with normalization   72.05980066445184\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------------- \n",
      "\n",
      "accuracy without normalization for cross validation =  0  ->  64.28571428571429\n",
      "Computation time: 0.03600144386291504\n",
      "Missing values:  {3.0: [1.0, 2.0, 1.0, 2.0], 2.0: [5.0, 1.0, 1.0, 1.0, 5.0, 1.0, 1.0, 1.0, 1.0], 1.0: [2.0], 6.0: [7.0]}\n",
      "\n",
      "accuracy with normalization for cross validation =  0  ->  66.66666666666666\n",
      "Computation time: 0.03900456428527832\n",
      "Missing values:  {1.0: [3.0, 2.0], 3.0: [1.0, 2.0, 1.0, 2.0], 2.0: [5.0, 1.0, 1.0, 5.0, 1.0, 1.0, 1.0, 1.0]}\n",
      "\n",
      "weighted accuracy without normalization for cross validation =  0  ->  61.904761904761905\n",
      "Computation time: 0.03601527214050293\n",
      "Missing values:  {1.0: [3.0, 2.0], 3.0: [1.0, 2.0, 1.0, 2.0], 2.0: [5.0, 1.0, 1.0, 1.0, 5.0, 1.0, 1.0, 1.0, 1.0], 6.0: [7.0]}\n",
      "\n",
      "weighted accuracy with normalization for cross validation =  0  ->  66.66666666666666\n",
      "Computation time: 0.03700876235961914\n",
      "Missing values:  {1.0: [3.0, 2.0], 3.0: [1.0, 2.0, 1.0, 2.0], 2.0: [5.0, 1.0, 1.0, 5.0, 1.0, 1.0, 1.0, 1.0]}\n",
      "\n",
      "accuracy without normalization for cross validation =  1  ->  71.42857142857143\n",
      "Computation time: 0.03599405288696289\n",
      "Missing values:  {3.0: [1.0, 1.0], 2.0: [6.0, 1.0, 1.0], 7.0: [1.0], 1.0: [2.0, 2.0, 2.0, 2.0, 3.0], 5.0: [7.0]}\n",
      "\n",
      "accuracy with normalization for cross validation =  1  ->  69.04761904761905\n",
      "Computation time: 0.036014556884765625\n",
      "Missing values:  {3.0: [1.0], 2.0: [6.0, 1.0, 1.0, 5.0, 5.0], 1.0: [3.0, 2.0, 2.0, 2.0, 2.0], 7.0: [1.0], 5.0: [7.0]}\n",
      "\n",
      "weighted accuracy without normalization for cross validation =  1  ->  71.42857142857143\n",
      "Computation time: 0.039014577865600586\n",
      "Missing values:  {3.0: [1.0, 1.0], 2.0: [6.0, 1.0, 1.0], 1.0: [3.0, 2.0, 2.0, 2.0, 3.0], 7.0: [1.0], 5.0: [7.0]}\n",
      "\n",
      "weighted accuracy with normalization for cross validation =  1  ->  73.80952380952381\n",
      "Computation time: 0.03599739074707031\n",
      "Missing values:  {2.0: [6.0, 1.0, 1.0, 5.0], 1.0: [3.0, 2.0, 2.0, 2.0, 2.0], 7.0: [1.0], 5.0: [7.0]}\n",
      "\n",
      "accuracy without normalization for cross validation =  2  ->  61.904761904761905\n",
      "Computation time: 0.036019325256347656\n",
      "Missing values:  {3.0: [1.0, 1.0, 1.0, 1.0], 1.0: [2.0, 2.0, 3.0], 2.0: [5.0, 6.0, 1.0], 6.0: [2.0], 5.0: [2.0, 2.0, 1.0], 7.0: [5.0, 2.0]}\n",
      "\n",
      "accuracy with normalization for cross validation =  2  ->  69.04761904761905\n",
      "Computation time: 0.03500771522521973\n",
      "Missing values:  {3.0: [1.0, 1.0, 1.0, 1.0], 1.0: [2.0, 2.0, 3.0], 2.0: [5.0, 1.0], 6.0: [5.0], 7.0: [5.0, 2.0], 5.0: [2.0]}\n",
      "\n",
      "weighted accuracy without normalization for cross validation =  2  ->  64.28571428571429\n",
      "Computation time: 0.036995649337768555\n",
      "Missing values:  {3.0: [1.0, 1.0, 1.0], 1.0: [2.0, 2.0, 3.0], 2.0: [5.0, 6.0, 1.0], 6.0: [2.0], 5.0: [2.0, 2.0, 1.0], 7.0: [5.0, 2.0]}\n",
      "\n",
      "weighted accuracy with normalization for cross validation =  2  ->  69.04761904761905\n",
      "Computation time: 0.03601431846618652\n",
      "Missing values:  {3.0: [1.0, 1.0, 1.0, 1.0], 1.0: [2.0, 2.0, 3.0], 2.0: [5.0, 1.0], 6.0: [5.0], 7.0: [5.0, 2.0], 5.0: [2.0]}\n",
      "\n",
      "accuracy without normalization for cross validation =  3  ->  76.19047619047619\n",
      "Computation time: 0.03600788116455078\n",
      "Missing values:  {5.0: [7.0], 2.0: [1.0, 1.0, 1.0, 3.0], 6.0: [7.0], 3.0: [1.0], 7.0: [6.0], 1.0: [2.0, 2.0]}\n",
      "\n",
      "accuracy with normalization for cross validation =  3  ->  83.33333333333334\n",
      "Computation time: 0.036014556884765625\n",
      "Missing values:  {2.0: [3.0, 1.0, 1.0], 3.0: [1.0], 7.0: [6.0], 1.0: [2.0], 6.0: [1.0]}\n",
      "\n",
      "weighted accuracy without normalization for cross validation =  3  ->  80.95238095238095\n",
      "Computation time: 0.03600168228149414\n",
      "Missing values:  {2.0: [1.0, 1.0, 1.0], 6.0: [7.0], 3.0: [1.0], 7.0: [6.0], 1.0: [2.0, 2.0]}\n",
      "\n",
      "weighted accuracy with normalization for cross validation =  3  ->  83.33333333333334\n",
      "Computation time: 0.03683042526245117\n",
      "Missing values:  {2.0: [3.0, 1.0, 1.0], 3.0: [1.0], 7.0: [6.0], 1.0: [2.0], 6.0: [1.0]}\n",
      "\n",
      "accuracy without normalization for cross validation =  4  ->  62.7906976744186\n",
      "Computation time: 0.036020755767822266\n",
      "Missing values:  {3.0: [1.0, 1.0], 1.0: [2.0, 2.0, 3.0, 2.0], 2.0: [1.0, 1.0, 1.0, 1.0, 3.0], 6.0: [5.0], 7.0: [6.0, 2.0, 2.0], 5.0: [2.0]}\n",
      "\n",
      "accuracy with normalization for cross validation =  4  ->  65.11627906976744\n",
      "Computation time: 0.03700828552246094\n",
      "Missing values:  {3.0: [1.0, 2.0], 2.0: [1.0, 1.0, 1.0, 1.0], 6.0: [5.0], 1.0: [2.0, 3.0, 3.0, 2.0], 7.0: [2.0, 2.0, 2.0], 5.0: [2.0]}\n",
      "\n",
      "weighted accuracy without normalization for cross validation =  4  ->  65.11627906976744\n",
      "Computation time: 0.03600764274597168\n",
      "Missing values:  {3.0: [1.0], 1.0: [2.0, 2.0, 3.0, 2.0], 2.0: [1.0, 1.0, 1.0, 1.0, 3.0], 6.0: [5.0], 7.0: [1.0, 2.0, 2.0], 5.0: [2.0]}\n",
      "\n",
      "weighted accuracy with normalization for cross validation =  4  ->  67.44186046511628\n",
      "Computation time: 0.03799605369567871\n",
      "Missing values:  {2.0: [1.0, 1.0, 1.0, 1.0], 6.0: [5.0], 1.0: [2.0, 3.0, 3.0, 2.0], 7.0: [2.0, 2.0, 2.0], 5.0: [2.0], 3.0: [2.0]}\n",
      "\n",
      "for k =  5\n",
      "average_accuracy   67.32004429678848\n",
      "average_accuracy with normalization   70.6423034330011\n",
      "average_accuracy_weighted without normalization   68.73754152823922\n",
      "average_accuracy_weighted with normalization   72.05980066445184\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------------- \n",
      "\n",
      "accuracy without normalization for cross validation =  0  ->  54.761904761904766\n",
      "Computation time: 0.03702116012573242\n",
      "Missing values:  {6.0: [7.0, 7.0], 1.0: [3.0, 2.0], 3.0: [1.0, 2.0, 1.0, 1.0], 2.0: [5.0, 1.0, 1.0, 1.0, 5.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}\n",
      "\n",
      "accuracy with normalization for cross validation =  0  ->  71.42857142857143\n",
      "Computation time: 0.035767316818237305\n",
      "Missing values:  {3.0: [1.0, 2.0, 1.0, 1.0], 1.0: [2.0], 2.0: [5.0, 6.0, 1.0, 5.0, 1.0, 1.0, 1.0]}\n",
      "\n",
      "weighted accuracy without normalization for cross validation =  0  ->  52.38095238095239\n",
      "Computation time: 0.036995649337768555\n",
      "Missing values:  {1.0: [3.0, 3.0, 2.0], 6.0: [7.0, 7.0], 3.0: [1.0, 2.0, 1.0, 1.0], 2.0: [5.0, 1.0, 1.0, 1.0, 5.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}\n",
      "\n",
      "weighted accuracy with normalization for cross validation =  0  ->  69.04761904761905\n",
      "Computation time: 0.03702116012573242\n",
      "Missing values:  {1.0: [3.0, 2.0], 3.0: [1.0, 2.0, 1.0, 1.0], 2.0: [5.0, 6.0, 1.0, 5.0, 1.0, 1.0, 1.0]}\n",
      "\n",
      "accuracy without normalization for cross validation =  1  ->  76.19047619047619\n",
      "Computation time: 0.03600788116455078\n",
      "Missing values:  {3.0: [1.0, 1.0], 2.0: [6.0, 1.0, 5.0], 7.0: [1.0], 1.0: [2.0, 2.0, 2.0], 5.0: [7.0]}\n",
      "\n",
      "accuracy with normalization for cross validation =  1  ->  69.04761904761905\n",
      "Computation time: 0.0370023250579834\n",
      "Missing values:  {3.0: [1.0, 2.0, 1.0], 2.0: [6.0, 1.0, 5.0, 5.0], 7.0: [1.0], 1.0: [2.0, 2.0, 2.0, 3.0], 5.0: [7.0]}\n",
      "\n",
      "weighted accuracy without normalization for cross validation =  1  ->  76.19047619047619\n",
      "Computation time: 0.03700828552246094\n",
      "Missing values:  {3.0: [1.0, 1.0], 2.0: [6.0, 1.0, 5.0], 7.0: [1.0], 1.0: [2.0, 2.0, 2.0], 5.0: [7.0]}\n",
      "\n",
      "weighted accuracy with normalization for cross validation =  1  ->  73.80952380952381\n",
      "Computation time: 0.03601408004760742\n",
      "Missing values:  {2.0: [6.0, 1.0, 5.0], 3.0: [2.0, 1.0], 7.0: [1.0], 1.0: [2.0, 2.0, 2.0, 3.0], 5.0: [7.0]}\n",
      "\n",
      "accuracy without normalization for cross validation =  2  ->  64.28571428571429\n",
      "Computation time: 0.03598833084106445\n",
      "Missing values:  {3.0: [1.0, 1.0, 1.0, 1.0], 1.0: [2.0, 2.0, 3.0], 2.0: [5.0, 6.0, 1.0], 6.0: [5.0], 5.0: [2.0, 2.0], 7.0: [5.0, 2.0]}\n",
      "\n",
      "accuracy with normalization for cross validation =  2  ->  61.904761904761905\n",
      "Computation time: 0.036008358001708984\n",
      "Missing values:  {3.0: [1.0, 1.0, 1.0, 2.0, 1.0], 1.0: [2.0, 2.0, 3.0], 2.0: [5.0, 6.0, 1.0], 6.0: [5.0], 7.0: [5.0, 2.0], 5.0: [2.0, 2.0]}\n",
      "\n",
      "weighted accuracy without normalization for cross validation =  2  ->  61.904761904761905\n",
      "Computation time: 0.03702139854431152\n",
      "Missing values:  {3.0: [1.0, 1.0, 1.0, 1.0], 1.0: [2.0, 2.0, 3.0], 2.0: [5.0, 6.0, 1.0], 6.0: [5.0], 5.0: [2.0, 2.0, 2.0], 7.0: [5.0, 2.0]}\n",
      "\n",
      "weighted accuracy with normalization for cross validation =  2  ->  61.904761904761905\n",
      "Computation time: 0.03598928451538086\n",
      "Missing values:  {3.0: [1.0, 1.0, 1.0, 2.0, 1.0], 1.0: [2.0, 2.0, 3.0], 2.0: [5.0, 6.0, 1.0], 6.0: [5.0], 7.0: [5.0, 2.0], 5.0: [2.0, 2.0]}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy without normalization for cross validation =  3  ->  78.57142857142857\n",
      "Computation time: 0.036020755767822266\n",
      "Missing values:  {5.0: [7.0], 2.0: [1.0, 1.0, 1.0], 6.0: [7.0], 3.0: [1.0], 7.0: [6.0], 1.0: [2.0, 2.0]}\n",
      "\n",
      "accuracy with normalization for cross validation =  3  ->  83.33333333333334\n",
      "Computation time: 0.036995887756347656\n",
      "Missing values:  {5.0: [7.0], 2.0: [1.0, 1.0], 6.0: [7.0], 3.0: [1.0], 7.0: [6.0], 1.0: [2.0]}\n",
      "\n",
      "weighted accuracy without normalization for cross validation =  3  ->  80.95238095238095\n",
      "Computation time: 0.036020755767822266\n",
      "Missing values:  {2.0: [1.0, 1.0, 1.0], 6.0: [7.0], 3.0: [1.0], 7.0: [6.0], 1.0: [2.0, 2.0]}\n",
      "\n",
      "weighted accuracy with normalization for cross validation =  3  ->  85.71428571428571\n",
      "Computation time: 0.035996198654174805\n",
      "Missing values:  {2.0: [1.0, 1.0], 6.0: [7.0], 3.0: [1.0], 7.0: [6.0], 1.0: [2.0]}\n",
      "\n",
      "accuracy without normalization for cross validation =  4  ->  60.46511627906976\n",
      "Computation time: 0.03801417350769043\n",
      "Missing values:  {3.0: [1.0, 1.0, 2.0], 1.0: [2.0, 2.0, 3.0, 2.0], 2.0: [1.0, 1.0, 1.0, 1.0, 3.0], 6.0: [5.0], 7.0: [1.0, 2.0, 2.0], 5.0: [2.0]}\n",
      "\n",
      "accuracy with normalization for cross validation =  4  ->  62.7906976744186\n",
      "Computation time: 0.03600144386291504\n",
      "Missing values:  {2.0: [1.0, 1.0, 1.0, 1.0], 6.0: [5.0], 1.0: [2.0, 3.0, 2.0, 2.0], 3.0: [1.0, 2.0], 7.0: [2.0, 2.0, 2.0], 5.0: [2.0, 7.0]}\n",
      "\n",
      "weighted accuracy without normalization for cross validation =  4  ->  60.46511627906976\n",
      "Computation time: 0.03800845146179199\n",
      "Missing values:  {3.0: [1.0, 1.0, 2.0], 1.0: [2.0, 2.0, 3.0, 2.0], 2.0: [1.0, 1.0, 1.0, 1.0, 3.0], 6.0: [5.0], 7.0: [1.0, 2.0, 2.0], 5.0: [2.0]}\n",
      "\n",
      "weighted accuracy with normalization for cross validation =  4  ->  65.11627906976744\n",
      "Computation time: 0.03699541091918945\n",
      "Missing values:  {2.0: [1.0, 1.0, 1.0, 1.0], 6.0: [5.0], 1.0: [2.0, 3.0, 2.0, 2.0], 3.0: [1.0, 2.0], 7.0: [2.0, 2.0, 2.0], 5.0: [2.0]}\n",
      "\n",
      "for k =  7\n",
      "average_accuracy   66.85492801771872\n",
      "average_accuracy with normalization   69.70099667774086\n",
      "average_accuracy_weighted without normalization   66.37873754152824\n",
      "average_accuracy_weighted with normalization   71.11849390919159\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------------- \n",
      "\n",
      "accuracy without normalization for cross validation =  0  ->  54.761904761904766\n",
      "Computation time: 0.03564739227294922\n",
      "Missing values:  {6.0: [7.0, 7.0], 3.0: [1.0, 2.0, 1.0, 1.0], 2.0: [5.0, 6.0, 1.0, 1.0, 1.0, 5.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 1.0: [2.0]}\n",
      "\n",
      "accuracy with normalization for cross validation =  0  ->  69.04761904761905\n",
      "Computation time: 0.0370023250579834\n",
      "Missing values:  {3.0: [1.0, 2.0, 1.0, 1.0], 1.0: [2.0], 2.0: [5.0, 6.0, 1.0, 5.0, 1.0, 1.0, 1.0, 1.0]}\n",
      "\n",
      "weighted accuracy without normalization for cross validation =  0  ->  54.761904761904766\n",
      "Computation time: 0.03701448440551758\n",
      "Missing values:  {6.0: [7.0, 7.0], 3.0: [1.0, 2.0, 1.0, 1.0], 2.0: [5.0, 6.0, 1.0, 1.0, 1.0, 5.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 1.0: [2.0]}\n",
      "\n",
      "weighted accuracy with normalization for cross validation =  0  ->  69.04761904761905\n",
      "Computation time: 0.03600811958312988\n",
      "Missing values:  {3.0: [1.0, 2.0, 1.0, 1.0], 1.0: [2.0], 2.0: [5.0, 6.0, 1.0, 5.0, 1.0, 1.0, 1.0, 1.0]}\n",
      "\n",
      "accuracy without normalization for cross validation =  1  ->  71.42857142857143\n",
      "Computation time: 0.03600788116455078\n",
      "Missing values:  {3.0: [1.0, 1.0], 2.0: [6.0, 1.0, 1.0, 5.0, 5.0], 7.0: [1.0], 1.0: [2.0, 2.0, 2.0], 5.0: [7.0]}\n",
      "\n",
      "accuracy with normalization for cross validation =  1  ->  71.42857142857143\n",
      "Computation time: 0.0367128849029541\n",
      "Missing values:  {3.0: [1.0, 2.0], 2.0: [6.0, 1.0, 5.0, 5.0], 7.0: [1.0], 1.0: [2.0, 2.0, 2.0, 3.0], 5.0: [7.0]}\n",
      "\n",
      "weighted accuracy without normalization for cross validation =  1  ->  73.80952380952381\n",
      "Computation time: 0.036014556884765625\n",
      "Missing values:  {3.0: [1.0, 1.0], 2.0: [6.0, 1.0, 1.0, 5.0], 7.0: [1.0], 1.0: [2.0, 2.0, 2.0], 5.0: [7.0]}\n",
      "\n",
      "weighted accuracy with normalization for cross validation =  1  ->  73.80952380952381\n",
      "Computation time: 0.03600192070007324\n",
      "Missing values:  {3.0: [1.0, 2.0], 2.0: [6.0, 1.0, 5.0], 7.0: [1.0], 1.0: [2.0, 2.0, 2.0, 3.0], 5.0: [7.0]}\n",
      "\n",
      "accuracy without normalization for cross validation =  2  ->  61.904761904761905\n",
      "Computation time: 0.035105228424072266\n",
      "Missing values:  {3.0: [1.0, 1.0, 1.0, 1.0], 1.0: [2.0, 3.0, 2.0], 2.0: [5.0, 6.0, 1.0], 6.0: [5.0], 5.0: [2.0, 2.0, 2.0], 7.0: [5.0, 2.0]}\n",
      "\n",
      "accuracy with normalization for cross validation =  2  ->  61.904761904761905\n",
      "Computation time: 0.03600049018859863\n",
      "Missing values:  {3.0: [1.0, 1.0, 1.0, 2.0, 1.0], 1.0: [2.0, 2.0, 2.0], 2.0: [5.0, 1.0], 6.0: [5.0], 5.0: [2.0, 2.0, 2.0], 7.0: [5.0, 2.0]}\n",
      "\n",
      "weighted accuracy without normalization for cross validation =  2  ->  61.904761904761905\n",
      "Computation time: 0.03600168228149414\n",
      "Missing values:  {3.0: [1.0, 1.0, 1.0, 1.0], 1.0: [2.0, 3.0, 2.0], 2.0: [5.0, 6.0, 1.0], 6.0: [5.0], 5.0: [2.0, 2.0, 2.0], 7.0: [5.0, 2.0]}\n",
      "\n",
      "weighted accuracy with normalization for cross validation =  2  ->  61.904761904761905\n",
      "Computation time: 0.036014556884765625\n",
      "Missing values:  {3.0: [1.0, 1.0, 1.0, 2.0, 1.0], 1.0: [2.0, 2.0, 2.0], 2.0: [5.0, 1.0], 6.0: [5.0], 5.0: [2.0, 2.0, 2.0], 7.0: [5.0, 2.0]}\n",
      "\n",
      "accuracy without normalization for cross validation =  3  ->  71.42857142857143\n",
      "Computation time: 0.03701448440551758\n",
      "Missing values:  {5.0: [7.0, 2.0], 2.0: [1.0, 1.0, 1.0], 1.0: [2.0, 2.0, 2.0], 6.0: [7.0, 1.0], 3.0: [1.0], 7.0: [6.0]}\n",
      "\n",
      "accuracy with normalization for cross validation =  3  ->  76.19047619047619\n",
      "Computation time: 0.03600811958312988\n",
      "Missing values:  {5.0: [7.0, 2.0, 2.0, 2.0], 2.0: [1.0, 1.0], 6.0: [7.0], 3.0: [1.0], 7.0: [6.0], 1.0: [2.0]}\n",
      "\n",
      "weighted accuracy without normalization for cross validation =  3  ->  76.19047619047619\n",
      "Computation time: 0.03500795364379883\n",
      "Missing values:  {2.0: [1.0, 1.0, 1.0], 1.0: [2.0, 2.0, 2.0], 6.0: [7.0], 5.0: [2.0], 3.0: [1.0], 7.0: [6.0]}\n",
      "\n",
      "weighted accuracy with normalization for cross validation =  3  ->  83.33333333333334\n",
      "Computation time: 0.03700828552246094\n",
      "Missing values:  {2.0: [1.0, 1.0], 6.0: [7.0], 3.0: [1.0], 7.0: [6.0], 5.0: [2.0], 1.0: [2.0]}\n",
      "\n",
      "accuracy without normalization for cross validation =  4  ->  58.139534883720934\n",
      "Computation time: 0.038976192474365234\n",
      "Missing values:  {3.0: [1.0, 1.0, 2.0], 1.0: [2.0, 2.0, 3.0, 2.0], 2.0: [1.0, 1.0, 1.0, 1.0, 1.0, 3.0], 6.0: [5.0], 7.0: [1.0, 2.0, 2.0], 5.0: [2.0]}\n",
      "\n",
      "accuracy with normalization for cross validation =  4  ->  65.11627906976744\n",
      "Computation time: 0.03600811958312988\n",
      "Missing values:  {2.0: [1.0, 1.0, 1.0, 1.0], 6.0: [5.0], 1.0: [2.0, 3.0, 2.0, 2.0], 3.0: [1.0, 2.0], 7.0: [2.0, 2.0], 5.0: [2.0, 7.0]}\n",
      "\n",
      "weighted accuracy without normalization for cross validation =  4  ->  62.7906976744186\n",
      "Computation time: 0.03600168228149414\n",
      "Missing values:  {3.0: [1.0, 2.0], 1.0: [2.0, 2.0, 3.0, 2.0], 2.0: [1.0, 1.0, 1.0, 1.0, 3.0], 6.0: [5.0], 7.0: [1.0, 2.0, 2.0], 5.0: [2.0]}\n",
      "\n",
      "weighted accuracy with normalization for cross validation =  4  ->  67.44186046511628\n",
      "Computation time: 0.03700852394104004\n",
      "Missing values:  {2.0: [1.0, 1.0, 1.0, 1.0], 6.0: [5.0], 1.0: [2.0, 3.0, 2.0, 2.0], 3.0: [1.0, 2.0], 7.0: [2.0, 2.0], 5.0: [2.0]}\n",
      "\n",
      "for k =  9\n",
      "average_accuracy   63.5326688815061\n",
      "average_accuracy with normalization   68.73754152823919\n",
      "average_accuracy_weighted without normalization   65.89147286821705\n",
      "average_accuracy_weighted with normalization   71.10741971207088\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "k_value_list = [1, 3, 5, 7, 9]\n",
    "average_accuracy = 0\n",
    "average_accuracy_normalization = 0\n",
    "average_accuracy_weighted = 0\n",
    "average_accuracy_weighted_normalization = 0\n",
    "for k in k_value_list:\n",
    "    for i in range(5):  # 5-fold cross validation\n",
    "        validate = int(dfSize * .2 * i)\n",
    "        if i == 4:\n",
    "            x_validate = features[validate:, :]\n",
    "            x_normalized_validate = normalized_features[validate:, :]\n",
    "            y_validate = labels[validate:]\n",
    "        else:\n",
    "            x_validate = features[validate:validate + numberOfCrossSize, :]\n",
    "            x_normalized_validate = normalized_features[validate:validate + numberOfCrossSize, :]\n",
    "            y_validate = labels[validate:validate + numberOfCrossSize]\n",
    "        x_train = features[validate + numberOfCrossSize:, :]\n",
    "        x_normalized_train = normalized_features[validate + numberOfCrossSize:, :]\n",
    "        y_train = labels[validate + numberOfCrossSize:]\n",
    "        if i != 0:\n",
    "            x_train2 = features[:validate, :]\n",
    "            x_normalized_train2 = normalized_features[:validate, :]\n",
    "            y_train2 = labels[:validate]\n",
    "\n",
    "            x_train = np.concatenate((x_train, x_train2), axis=0)\n",
    "            x_normalized_train = np.concatenate((x_normalized_train, x_normalized_train2), axis=0)\n",
    "            y_train = np.concatenate((y_train, y_train2), axis=0)\n",
    "\n",
    "\n",
    "        # predict their classes using kNN without feature normalization\n",
    "        start = time.time()\n",
    "        model = KNN_Classification(k)\n",
    "        model.fit(x_train, y_train)\n",
    "        predictions = model.predict(x_validate)\n",
    "        end = time.time()\n",
    "\n",
    "        accuracy = (np.sum(predictions == y_validate) / len(y_validate)) * 100\n",
    "        average_accuracy += accuracy\n",
    "        print(\"accuracy without normalization for cross validation = \", i, \" -> \", accuracy)\n",
    "        print(\"Computation time:\", end - start)\n",
    "\n",
    "        dictForMissingValues = {}\n",
    "        for count in range(len(predictions)):\n",
    "            if predictions[count] != y_validate[count]:\n",
    "                if y_validate[count] not in dictForMissingValues.keys():\n",
    "                    dictForMissingValues[y_validate[count]] = list()\n",
    "                    dictForMissingValues[y_validate[count]].append(predictions[count])\n",
    "                else:\n",
    "                    dictForMissingValues[y_validate[count]].append(predictions[count])\n",
    "\n",
    "        print(\"Missing values: \", dictForMissingValues)\n",
    "        print()\n",
    "\n",
    "        # predict their classes using kNN with feature normalization\n",
    "        start = time.time()\n",
    "        model = KNN_Classification(k)\n",
    "        model.fit(x_normalized_train, y_train)\n",
    "        predictions = model.predict(x_normalized_validate)\n",
    "        end = time.time()\n",
    "\n",
    "        accuracy = (np.sum(predictions == y_validate) / len(y_validate)) * 100\n",
    "        average_accuracy_normalization += accuracy\n",
    "        print(\"accuracy with normalization for cross validation = \", i, \" -> \", accuracy)\n",
    "        print(\"Computation time:\", end - start)\n",
    "\n",
    "        dictForMissingValues = {}\n",
    "        for count in range(len(predictions)):\n",
    "            if predictions[count] != y_validate[count]:\n",
    "                if y_validate[count] not in dictForMissingValues.keys():\n",
    "                    dictForMissingValues[y_validate[count]] = list()\n",
    "                    dictForMissingValues[y_validate[count]].append(predictions[count])\n",
    "                else:\n",
    "                    dictForMissingValues[y_validate[count]].append(predictions[count])\n",
    "\n",
    "\n",
    "        print(\"Missing values: \", dictForMissingValues)\n",
    "        print()\n",
    "\n",
    "        # predict their classes using weighted kNN without feature normalization\n",
    "        start = time.time()\n",
    "        model = KNN_Classification(k)\n",
    "        model.fit(x_train, y_train)\n",
    "        predictions = model.predict_weighted(x_validate, 0.1)\n",
    "        end = time.time()\n",
    "\n",
    "        accuracy = (np.sum(predictions == y_validate) / len(y_validate)) * 100\n",
    "        average_accuracy_weighted += accuracy\n",
    "        print(\"weighted accuracy without normalization for cross validation = \", i, \" -> \", accuracy)\n",
    "        print(\"Computation time:\", end - start)\n",
    "\n",
    "        dictForMissingValues = {}\n",
    "        for count in range(len(predictions)):\n",
    "            if predictions[count] != y_validate[count]:\n",
    "                if y_validate[count] not in dictForMissingValues.keys():\n",
    "                    dictForMissingValues[y_validate[count]] = list()\n",
    "                    dictForMissingValues[y_validate[count]].append(predictions[count])\n",
    "                else:\n",
    "                    dictForMissingValues[y_validate[count]].append(predictions[count])\n",
    "\n",
    "\n",
    "        print(\"Missing values: \", dictForMissingValues)\n",
    "        print()\n",
    "\n",
    "\n",
    "        # predict their classes using weighted kNN with feature normalization\n",
    "        start = time.time()\n",
    "        model = KNN_Classification(k)\n",
    "        model.fit(x_normalized_train, y_train)\n",
    "        predictions = model.predict_weighted(x_normalized_validate, 0.1)\n",
    "        end = time.time()\n",
    "\n",
    "        accuracy = (np.sum(predictions == y_validate) / len(y_validate)) * 100\n",
    "        average_accuracy_weighted_normalization += accuracy\n",
    "        print(\"weighted accuracy with normalization for cross validation = \", i, \" -> \", accuracy)\n",
    "        print(\"Computation time:\", end - start)\n",
    "\n",
    "        dictForMissingValues = {}\n",
    "        for count in range(len(predictions)):\n",
    "            if predictions[count] != y_validate[count]:\n",
    "                if y_validate[count] not in dictForMissingValues.keys():\n",
    "                    dictForMissingValues[y_validate[count]] = list()\n",
    "                    dictForMissingValues[y_validate[count]].append(predictions[count])\n",
    "                else:\n",
    "                    dictForMissingValues[y_validate[count]].append(predictions[count])\n",
    "\n",
    "\n",
    "        print(\"Missing values: \", dictForMissingValues)\n",
    "        print()\n",
    "\n",
    "\n",
    "        if i == 4:\n",
    "            print(\"for k = \" , k)\n",
    "            print(\"average_accuracy  \",  average_accuracy/ 5)\n",
    "            average_accuracy = 0\n",
    "            print(\"average_accuracy with normalization  \", average_accuracy_normalization / 5)\n",
    "            average_accuracy_normalization = 0\n",
    "            print(\"average_accuracy_weighted without normalization  \", average_accuracy_weighted / 5)\n",
    "            average_accuracy_weighted = 0\n",
    "            print(\"average_accuracy_weighted with normalization  \", average_accuracy_weighted_normalization / 5)\n",
    "            average_accuracy_weighted_normalization = 0\n",
    "            print()\n",
    "            print(\"\\n---------------------------------------------------------------------------------------- \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733da817",
   "metadata": {},
   "source": [
    "## Error Analysis for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485d123b",
   "metadata": {},
   "source": [
    "<img src=\"Part1.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06563c35",
   "metadata": {},
   "source": [
    "#### Find a few misclassified samples and comment on why you think they were hard to classify."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a64cff6",
   "metadata": {},
   "source": [
    "- Each missing value is printed on the screen. We see some similarities between the causes of missing values. Let's take label 2 for example. The 2 label is generally confused with 1 and 5. This may be because vectors labeled 2 and vectors labeled 1 and 5 are located close in space. In another example, let's look at the label 1. In general, we can easily see that 2 predictions are made where 1 should be. We have seen that the prediction of 1 is frequently made for the 2 labels. This can confirm what we have just done that the positions of the vectors are close to each other. <br> If the k value we are considering is more than the optimum k value, the model may have been overfitted. If the selected k value is less than the optimum k value, then the model may be looking at only the nearest ones, confusing the generic label because it is examining less data. Also, KNN is a lazy learner algorithm because it does not learn from the training set immediately instead it stores the dataset and at the time of classification, it performs an action on the dataset. For this reason, if our training set is not well mixed and there are not enough samples to be estimated, the model may fail in the estimated value. <br>\n",
    "In addition, missing values may also be due to lack of normalization. In the absence of normalization, some features may affect the model more than other features and mislead the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298ce996",
   "metadata": {},
   "source": [
    "#### Compare performance of different feature normalization choices and investigate the effect of important system parameters (number of training samples used, k in k-NN, etc.). Wherever relevant, feel free to discuss computation time in addition to classification rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79d259c",
   "metadata": {},
   "source": [
    "- At the table, if we compare the weighted and not weighted results at the same K and normalized values, at the most of the case we see the weighted KNN is giving more efficient results. This is beacuse weighted KNN provides more sensitive mesurement with calculating the distance of neighbors. Scoring the neigbours with their weights is make the results more efficent. This is because we give the greatest weight to the nearest neighbor. Since the weight is equal to 1/d, the weight decreases as the distance increases and the farthest point has the least effect on the point being estimated. \n",
    "<br>\n",
    "- Sometimes a dataset can contain extreme values  which name is outliers that are outside the range of what is expected and unlike the other data. In additions, the ranges of some of features in dataset can differ greatly from the ranges of other featurs. This causes the feature with a wide range to be more effective in estimating the model, may be resulting in incorrect estimation. We can get rid of this problems with normalization.  The normalization here is done by  rescaled to the 0-1 interval is done by shifted the values of each feature so that the minimal value is 0, and then divided by the maximal value. At the table if we compare the normalized and not normalized values at the average, we seenormalized values has better results all the cases. The reason is data set has different features that has various intervals and data set has outlier values. When we determine a fixed interval this allows the evaluating the data more accurate.\n",
    "<br>\n",
    "- As considering the table we might say the best K value is 1. If we check the other K values we can't see a linear ratio on the results. The accuracy rate decreasing at K = 3 and increasing at K = 5 then continuing the decreasing with upper K values. If we look at the results in the table, k=1 gives us the best accuracy value for this problem.\n",
    "<br>\n",
    "- On the other hand we see the increasing k values effect the calculation time of program. As the k value increased and weighted operation was performed at the same time, the prediction time of the model became longer. This is due to additional processing, but this time is in milliseconds and the difference is quite small.\n",
    "<br>\n",
    "- In short, normalization on the data increased the accuracy value. The use of Weighted KNN increased the accuracy value compared to the use of the normal KNN algorithm. Based on the data we have, 1 seems to be the best k-value to choose from. If an estimation is desired, the model with normalized, weighted knn algorithm and k value of 1 should be preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c5510c",
   "metadata": {},
   "source": [
    "#   <font color=blue>PART 2: Concrete Material Strength Estimation from Data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd016fc",
   "metadata": {},
   "source": [
    "## Abstract :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f3347d",
   "metadata": {},
   "source": [
    "In the second part of this project, the KNN machine learning algorithm was used for the concrete material strength dataset. The accuracies of the model were calculated on 5-fold cross validation with different k parameters (1,3,5,7,9) of the model's different k-NN and weighted k-NN  models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83345fbd",
   "metadata": {},
   "source": [
    "## Code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d2ec1d",
   "metadata": {},
   "source": [
    "Required libraries are imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7f5db9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c7e0ca",
   "metadata": {},
   "source": [
    "KNN Regression Class  \n",
    "\n",
    "1. \"fit(self, x, y)\" function: Takes which features to train and their labels as parameters.  \n",
    "\n",
    "2. \"predict(self, test_validation)\" function: It calls the _predict(self,x) function for each data to be tested and stores the predictions in an array. This returns the predicted array.\n",
    "\n",
    "3. \"predict_weighted(self, test_validation)\" function: It calls the _predict_weighted(self,x,hyper_parameter) function for each data to be tested and stores the predictions in an array. This returns the predicted array.\n",
    "\n",
    "4. \"euclidean_distance(self, row1, row2)\" function: Calculates the distance between two given vectors with Euclidean distance.\n",
    "\n",
    "5. \"manhattan_distance(self, row1, row2)\" function: Calculates the distance between two given vectors with Manahttan distance.\n",
    "\n",
    "6. \"_predict(self, x)\" function: With distance array, for each sample in the given training set, it holds the distance of a particular sample from other samples. And then, create k_indices array which holding the indexes of the vectors with the closest distance(as many as k). Next, create k_nearest_labels array which holding the values of the vectors with the closest distance. Then, the values of the nearest neighbors are summed up and divided by k, that is, they are averaged. The result is returned.\n",
    "\n",
    "7. \"_predict_weighted(self, x , hyper_parameter)\" function: With distance array, for each sample in the given training set, it holds the distance of a particular sample from other samples. And then, create k_indices array which holding the indexes of the vectors with the closest distance(as many as k). And then, create k_nearest_labels array which holding the values of the vectors with the closest distance.Also, create k_distances array which holding the distances of the vectors with the closest distance. And create a dictionary to match distances and labels. The hyper parameter was created to avoid offsets of 0.0. We can explain it like this, if the distance is zero, the 1/d calculation cannot be made and in fact the closest value is considered invalid. For this reason, the distance measure as much as the entered hyper parameter is added to all the distances in the data set. In this way, the closest value actually has the shortest distance and has the greatest weight through the 1/d expression. After adding the entered hyperparameter to all distances, the corresponding weights(1/d) for all labels were calculated and recorded in a dictionary. Then, label values that is keys of dictionary, ​​and weights  are multiplied and assigned to the variable val. At the same time, only the weights are added and assigned to the variable divided. val/divided is result. The result is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "86b7846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN_Regression:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.x_train = x\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, test_validation):\n",
    "        predicted_labels = [self._predict(x) for x in test_validation]\n",
    "        return np.array(predicted_labels)\n",
    "\n",
    "    def predict_weighted(self, test_validation , hyper_parameter):\n",
    "        predicted_labels = [self._predict_weighted(x, hyper_parameter) for x in test_validation]\n",
    "        return np.array(predicted_labels)\n",
    "\n",
    "    def euclidean_distance(self, row1, row2):\n",
    "        distance = 0.0\n",
    "        for i in range(len(row1) - 1):\n",
    "            distance += (row1[i] - row2[i]) ** 2\n",
    "        return sqrt(distance)\n",
    "\n",
    "    def manhattan_distance(self, row1, row2):\n",
    "        distance = 0.0\n",
    "        for i in range(len(row1) - 1):\n",
    "            distance += abs(row1[i] - row2[i])\n",
    "        return distance\n",
    "\n",
    "\n",
    "    def _predict(self, x):\n",
    "        distances = [self.euclidean_distance(x, x_train) for x_train in self.x_train]\n",
    "        # distances = [self.manhattan_distance(x, x_train) for x_train in self.x_train]\n",
    "\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "\n",
    "        average_value = 0\n",
    "        for i in k_nearest_labels:\n",
    "            average_value += i\n",
    "        average_value = average_value / len(k_nearest_labels)\n",
    "        return average_value\n",
    "\n",
    "    def _predict_weighted(self, x, hyper_parameter):\n",
    "        distances = [self.euclidean_distance(x, x_train) for x_train in self.x_train]\n",
    "        # distances = [self.manhattan_distance(x, x_train) for x_train in self.x_train]\n",
    "\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        k_distances = [distances[i] for i in k_indices]\n",
    "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "\n",
    "        dict = {}\n",
    "\n",
    "        #add a small value in all label's distance and avoid 0 distance\n",
    "        for i in range(len(k_distances)):\n",
    "            k_distances[i] = k_distances[i] + hyper_parameter\n",
    "\n",
    "        for i in range(len(k_nearest_labels)):\n",
    "            if k_nearest_labels[i] in dict:\n",
    "                dict[k_nearest_labels[i]] += 1 / k_distances[i]\n",
    "            elif k_nearest_labels[i] not in dict:\n",
    "                dict[k_nearest_labels[i]] = 1 / k_distances[i]\n",
    "        val = 0\n",
    "        divided = 0\n",
    "\n",
    "        for i in dict.keys():\n",
    "            val += i * dict[i]\n",
    "            divided += dict[i]\n",
    "\n",
    "        return val / divided\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3e2aad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading csv file with pandas\n",
    "df = pd.read_csv('Concrete_Data_Yeh.csv')\n",
    "\n",
    "# parse the data and convert numpy\n",
    "df = df.to_numpy()\n",
    "\n",
    "#shuffle\n",
    "np.random.shuffle(df)\n",
    "\n",
    "# parse the data as features and labels\n",
    "features = df[:, :-1]\n",
    "labels = df[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059e3a56",
   "metadata": {},
   "source": [
    "Used min-max normalization on the features of samples to re-scale each feature (feature/attribute column on data) between (0-1) range. For this, each column was handled separately, the minimum and maximum data in the column were obtained, and the data with the normalization algorithm were re-recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "12fa3784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(df, features):\n",
    "    for feature in range(df.shape[1] - 1):\n",
    "        minvalue = features[:, feature].min()\n",
    "        maxvalue = features[:, feature].max()\n",
    "        for i in range(len(df)):\n",
    "            features[i, feature] = (features[i, feature] - minvalue) / (maxvalue - minvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998a2896",
   "metadata": {},
   "source": [
    "The mean absolute error is the sum of the absolute error value, a more direct representation of the sum of the error terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c3090dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanAbsoluteError(actual, predicted):\n",
    "    mae = 0\n",
    "    for i in range(len(actual)):\n",
    "        mae += abs(actual[i]-predicted[i])\n",
    "    return mae/len(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7acbf02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating normalized features\n",
    "normalized_features = features.copy()\n",
    "normalization(df, normalized_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c21abcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSize = len(df)  \n",
    "numberOfCrossSize = int(len(df) / 5)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae1e96e",
   "metadata": {},
   "source": [
    "For each desired k value (1,3,5,7,9), 5-fold cross validation was performed separately and the results were printed. First of all, validate and train sets were determined. Then, \"unnormalized and unweighted knn\" , \"normalized and unweighted knn\" , \"unnormalized and weighted knn\" and \"normalized and weighted knn\" algorithms were run with the training data set, respectively, and their mae values were reached. Each mae value and its average are printed on the screen.\n",
    "\n",
    "In the creation phase of each model, first the model was created with the k value, the training data was given to the model, and then it was expected to make predictions from the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1b3d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae without normalization for cross validation =  1  ->  12.212281553398055\n",
      "Computation time: 0.7358636856079102\n",
      "mae with normalization for cross validation =  1  ->  11.518640776699032\n",
      "Computation time: 0.7356243133544922\n",
      "weighted mae without normalization for cross validation =  1  ->  12.212281553398055\n",
      "Computation time: 0.7449262142181396\n",
      "weighted mae with normalization for cross validation =  1  ->  11.518640776699032\n",
      "Computation time: 0.7373499870300293\n",
      "mae without normalization for cross validation =  2  ->  11.693543689320386\n",
      "Computation time: 0.7461621761322021\n",
      "mae with normalization for cross validation =  2  ->  11.347864077669907\n",
      "Computation time: 0.7346677780151367\n",
      "weighted mae without normalization for cross validation =  2  ->  11.693543689320386\n",
      "Computation time: 0.7367873191833496\n",
      "weighted mae with normalization for cross validation =  2  ->  11.347864077669907\n",
      "Computation time: 0.7381596565246582\n",
      "mae without normalization for cross validation =  3  ->  12.23936893203884\n",
      "Computation time: 0.7371535301208496\n",
      "mae with normalization for cross validation =  3  ->  11.982572815533983\n",
      "Computation time: 0.7418229579925537\n",
      "weighted mae without normalization for cross validation =  3  ->  12.239368932038841\n",
      "Computation time: 0.7378818988800049\n",
      "weighted mae with normalization for cross validation =  3  ->  11.982572815533983\n",
      "Computation time: 0.7351593971252441\n",
      "mae without normalization for cross validation =  4  ->  12.544368932038836\n",
      "Computation time: 0.7351710796356201\n",
      "mae with normalization for cross validation =  4  ->  11.929174757281555\n",
      "Computation time: 0.739126443862915\n",
      "weighted mae without normalization for cross validation =  4  ->  12.544368932038836\n",
      "Computation time: 0.737576961517334\n",
      "weighted mae with normalization for cross validation =  4  ->  11.929174757281555\n",
      "Computation time: 0.7351653575897217\n",
      "mae without normalization for cross validation =  5  ->  11.795922330097099\n",
      "Computation time: 0.7375986576080322\n",
      "mae with normalization for cross validation =  5  ->  12.64504854368932\n",
      "Computation time: 0.7376246452331543\n",
      "weighted mae without normalization for cross validation =  5  ->  11.795922330097099\n",
      "Computation time: 0.7361660003662109\n",
      "weighted mae with normalization for cross validation =  5  ->  12.645048543689324\n",
      "Computation time: 0.7386553287506104\n",
      "for k =  1\n",
      "average_mae  12.097097087378643\n",
      "average_mae with normalization  11.88466019417476\n",
      "weighted average_mae without normalization  12.097097087378645\n",
      "weighted average_mae with normalization  11.884660194174762\n",
      "\n",
      "mae without normalization for cross validation =  1  ->  10.858462783171525\n",
      "Computation time: 0.747154712677002\n",
      "mae with normalization for cross validation =  1  ->  10.035663430420717\n",
      "Computation time: 0.7389822006225586\n",
      "weighted mae without normalization for cross validation =  1  ->  10.298372278719189\n",
      "Computation time: 0.7411379814147949\n",
      "weighted mae with normalization for cross validation =  1  ->  9.946050401294253\n",
      "Computation time: 0.7441530227661133\n",
      "mae without normalization for cross validation =  2  ->  10.603333333333332\n",
      "Computation time: 0.7405214309692383\n",
      "mae with normalization for cross validation =  2  ->  10.741941747572818\n",
      "Computation time: 0.7361509799957275\n",
      "weighted mae without normalization for cross validation =  2  ->  10.424130679328243\n",
      "Computation time: 0.7431674003601074\n",
      "weighted mae with normalization for cross validation =  2  ->  10.605784051545928\n",
      "Computation time: 0.739330530166626\n",
      "mae without normalization for cross validation =  3  ->  10.287540453074437\n",
      "Computation time: 0.7365069389343262\n",
      "mae with normalization for cross validation =  3  ->  10.581326860841429\n",
      "Computation time: 0.7342238426208496\n",
      "weighted mae without normalization for cross validation =  3  ->  10.37614938645357\n",
      "Computation time: 0.7358894348144531\n",
      "weighted mae with normalization for cross validation =  3  ->  10.47497281783634\n",
      "Computation time: 0.748772382736206\n",
      "mae without normalization for cross validation =  4  ->  10.71255663430421\n",
      "Computation time: 0.7483203411102295\n",
      "mae with normalization for cross validation =  4  ->  9.8856148867314\n",
      "Computation time: 0.7681732177734375\n",
      "weighted mae without normalization for cross validation =  4  ->  10.629182563555498\n",
      "Computation time: 0.7661817073822021\n",
      "weighted mae with normalization for cross validation =  4  ->  9.944293210096479\n",
      "Computation time: 0.7398290634155273\n",
      "mae without normalization for cross validation =  5  ->  11.076731391585763\n",
      "Computation time: 0.740537166595459\n",
      "mae with normalization for cross validation =  5  ->  11.368705501618127\n",
      "Computation time: 0.7692623138427734\n",
      "weighted mae without normalization for cross validation =  5  ->  10.94893178202965\n",
      "Computation time: 0.775458574295044\n",
      "weighted mae with normalization for cross validation =  5  ->  11.248540313447185\n",
      "Computation time: 0.7542366981506348\n",
      "for k =  3\n",
      "average_mae  10.707724919093854\n",
      "average_mae with normalization  10.522650485436898\n",
      "weighted average_mae without normalization  10.53535333801723\n",
      "weighted average_mae with normalization  10.443928158844038\n",
      "\n",
      "mae without normalization for cross validation =  1  ->  10.11998058252427\n",
      "Computation time: 0.7454392910003662\n",
      "mae with normalization for cross validation =  1  ->  9.622922330097085\n",
      "Computation time: 0.7374207973480225\n",
      "weighted mae without normalization for cross validation =  1  ->  10.04415570736043\n",
      "Computation time: 0.7441587448120117\n",
      "weighted mae with normalization for cross validation =  1  ->  9.730051168793075\n",
      "Computation time: 0.7409811019897461\n",
      "mae without normalization for cross validation =  2  ->  10.074621359223304\n",
      "Computation time: 0.7415003776550293\n",
      "mae with normalization for cross validation =  2  ->  10.15318446601942\n",
      "Computation time: 0.7393455505371094\n",
      "weighted mae without normalization for cross validation =  2  ->  10.082603901831698\n",
      "Computation time: 0.7393434047698975\n",
      "weighted mae with normalization for cross validation =  2  ->  10.080248828914222\n",
      "Computation time: 0.7436871528625488\n",
      "mae without normalization for cross validation =  3  ->  10.53121359223301\n",
      "Computation time: 0.7414753437042236\n",
      "mae with normalization for cross validation =  3  ->  10.776786407766993\n",
      "Computation time: 0.7420430183410645\n",
      "weighted mae without normalization for cross validation =  3  ->  10.491712570876718\n",
      "Computation time: 0.7425069808959961\n",
      "weighted mae with normalization for cross validation =  3  ->  10.475119586476593\n",
      "Computation time: 0.7390711307525635\n",
      "mae without normalization for cross validation =  4  ->  9.918368932038826\n",
      "Computation time: 0.7415416240692139\n",
      "mae with normalization for cross validation =  4  ->  9.691009708737868\n",
      "Computation time: 0.7387912273406982\n",
      "weighted mae without normalization for cross validation =  4  ->  10.435564359478843\n",
      "Computation time: 0.7439277172088623\n"
     ]
    }
   ],
   "source": [
    "k_value_list = [1,3,5,7,9]\n",
    "average_mae = 0\n",
    "average_mae_normalization = 0\n",
    "average_mae_weighted = 0\n",
    "average_mae_weighted_normalization = 0\n",
    "for k in k_value_list:\n",
    "    for i in range(5):  # 5-fold cross validation\n",
    "        validate = int(dfSize * .2 * i)\n",
    "        if i == 4:\n",
    "            x_validate = features[validate:, :]\n",
    "            x_normalized_validate = normalized_features[validate:, :]\n",
    "            y_validate = labels[validate:]\n",
    "        else:\n",
    "            x_validate = features[validate:validate + numberOfCrossSize, :]\n",
    "            x_normalized_validate = normalized_features[validate:validate + numberOfCrossSize, :]\n",
    "            y_validate = labels[validate:validate + numberOfCrossSize]\n",
    "        x_train = features[validate + numberOfCrossSize:, :]\n",
    "        x_normalized_train = normalized_features[validate + numberOfCrossSize:, :]\n",
    "        y_train = labels[validate + numberOfCrossSize:]\n",
    "        if i != 0:\n",
    "            x_train2 = features[:validate, :]\n",
    "            x_normalized_train2 = normalized_features[:validate, :]\n",
    "            y_train2 = labels[:validate]\n",
    "\n",
    "            x_train = np.concatenate((x_train, x_train2), axis=0)\n",
    "            x_normalized_train = np.concatenate((x_normalized_train, x_normalized_train2), axis=0)\n",
    "            y_train = np.concatenate((y_train, y_train2), axis=0)\n",
    "\n",
    "        # predict their continuous values using kNN without feature normalization\n",
    "        start = time.time()\n",
    "        model = KNN_Regression(k)\n",
    "        model.fit(x_train, y_train)\n",
    "        predictions = model.predict(x_validate)\n",
    "        end = time.time()\n",
    "\n",
    "        mae = meanAbsoluteError(y_validate, predictions)\n",
    "        print(\"mae without normalization for cross validation = \", i + 1, \" -> \", mae)\n",
    "        print(\"Computation time:\", end - start)\n",
    "        average_mae += mae\n",
    "\n",
    "\n",
    "        # predict their continuous values using kNN with feature normalization\n",
    "        start = time.time()\n",
    "        model = KNN_Regression(k)\n",
    "        model.fit(x_normalized_train, y_train)\n",
    "        predictions = model.predict(x_normalized_validate)\n",
    "        end = time.time()\n",
    "\n",
    "        mae = meanAbsoluteError(y_validate, predictions)\n",
    "        print(\"mae with normalization for cross validation = \", i+1, \" -> \", mae)\n",
    "        print(\"Computation time:\", end - start)\n",
    "        average_mae_normalization += mae\n",
    "\n",
    "\n",
    "        # predict their classes using weighted kNN without feature normalization\n",
    "        start = time.time()\n",
    "        model = KNN_Regression(k)\n",
    "        model.fit(x_train, y_train)\n",
    "        predictions = model.predict_weighted(x_validate, 0.1)\n",
    "        end = time.time()\n",
    "\n",
    "        mae = meanAbsoluteError(y_validate, predictions)\n",
    "        print(\"weighted mae without normalization for cross validation = \", i + 1, \" -> \", mae)\n",
    "        print(\"Computation time:\", end - start)\n",
    "        average_mae_weighted += mae\n",
    "      \n",
    "\n",
    "        # predict their continuous values using weighted kNN with feature normalization\n",
    "        start = time.time()\n",
    "        model = KNN_Regression(k)\n",
    "        model.fit(x_normalized_train, y_train)\n",
    "        predictions = model.predict_weighted(x_normalized_validate, 0.1)\n",
    "        end = time.time()\n",
    "\n",
    "        mae = meanAbsoluteError(y_validate, predictions)\n",
    "        print(\"weighted mae with normalization for cross validation = \", i + 1, \" -> \", mae)\n",
    "        print(\"Computation time:\", end - start)\n",
    "        average_mae_weighted_normalization += mae\n",
    "\n",
    "\n",
    "        if i == 4:\n",
    "            print(\"for k = \", k)\n",
    "            print(\"average_mae \", average_mae / 5)\n",
    "            average_mae = 0\n",
    "\n",
    "            print(\"average_mae with normalization \", average_mae_normalization / 5)\n",
    "            average_mae_normalization = 0\n",
    "\n",
    "            print(\"weighted average_mae without normalization \", average_mae_weighted / 5)\n",
    "            average_mae_weighted = 0\n",
    "\n",
    "            print(\"weighted average_mae with normalization \", average_mae_weighted_normalization / 5)\n",
    "            average_mae_weighted_normalization = 0\n",
    "\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804f0461",
   "metadata": {},
   "source": [
    "## Error Analysis for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8772f26f",
   "metadata": {},
   "source": [
    "<img src=\"Part2.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9901a265",
   "metadata": {},
   "source": [
    "#### Compare performance of different feature normalization choices and investigate the effect of important system parameters (number of training samples used, k in k-NN, etc.). Wherever relevant, feel free to discuss computation time in addition to regression/estimation rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150461e7",
   "metadata": {},
   "source": [
    "- At the table, if we compare the weighted and not weighted results at the same K and normalized values, we see similar results in weighted and unweighted KNN algorithms. This may be because when the values in the dataset are vectorized, the distance does not affect the result much.\n",
    "<br>\n",
    "- The normalization here is done by  rescaled to the 0-1 interval is done by shifted the values of each feature so that the minimal value is 0, and then divided by the maximal value. At the table if we compare the normalized and not normalized values at the average, we seenormalized values has better results all the cases but this difference is small. The reason may be data set has different features that has various intervals and data set has outlier values. When we determine a fixed interval this allows the evaluating the data more accurate. Also, when the dataset is examined, we see that some attributes have a high range, while others have a small range between 0 and 1. This indicates to us that normalization should be done in this data set.\n",
    "<br>\n",
    "- As considering the table we might say the best K value is 7. The error value, which decreased until the k value was 7, then increased. If we look at the results in the table, k=7 gives us the lowest error value for this problem.\n",
    "<br>\n",
    "- On the other hand we see the increasing k values effect the calculation time of program. As the k value increased and weighted operation was performed at the same time, the prediction time of the model became longer. This is due to additional processing, but this time is in milliseconds and the difference is quite small.\n",
    "<br>\n",
    "- In short, normalization on the data reduced the error value. The use of Weighted KNN reduced the error value compared to the use of the normal KNN algorithm. Based on the data we have, 7 seems to be the best k-value to choose from. If an estimation is desired, the model with normalized, weighted knn algorithm and k value of 7 should be preferred."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
